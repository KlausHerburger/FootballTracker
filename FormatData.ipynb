{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5f1ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "286ee975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoToImages(videoNr, scale_percent):\n",
    "    \"\"\" Gets an array with all image frames of a given video\n",
    "    Args:\n",
    "      videoNr: Number of the video to get the images for\n",
    "      scale_percent: Scale factor to reduce the image for performance reasons\n",
    "      \n",
    "    Returns:\n",
    "      Array with all image frames\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calcuate dimensions of scaled images\n",
    "    width = int(1920 * scale_percent / 100)\n",
    "    height = int(1088 * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    \n",
    "    # Load Video\n",
    "    video = cv2.VideoCapture('Data/filmrole' + str(videoNr) + '.avi')\n",
    "    if video.isOpened() == False:\n",
    "        print('Error opening the video file filmrole' + str(videoNr) + '.avi')\n",
    "        \n",
    "    # Iterate through all image frames saving the resized frame\n",
    "    success,image = video.read()\n",
    "    images = np.zeros((3000,height,width,3),dtype=np.uint8)\n",
    "    count = 0\n",
    "    while success:\n",
    "        resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "        images[count] = resized\n",
    "        success,image = video.read()\n",
    "        count = count+1\n",
    "        \n",
    "    return images[0:count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0356c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2998, 108, 192, 3)\n",
      "Video 1_10 saved\n",
      "(2998, 108, 192, 3)\n",
      "Video 2_10 saved\n",
      "(2999, 108, 192, 3)\n",
      "Video 3_10 saved\n",
      "(2999, 108, 192, 3)\n",
      "Video 4_10 saved\n",
      "(2999, 108, 192, 3)\n",
      "Video 5_10 saved\n",
      "(3000, 108, 192, 3)\n",
      "Video 6_10 saved\n"
     ]
    }
   ],
   "source": [
    "# Save images for all six videos in serparate numpy arrays, where all frames are reduced to 10% of their size for performance reasons\n",
    "\n",
    "for i in [1,2,3,4,5,6]:\n",
    "    for p in [10]:\n",
    "        imagesVideo = videoToImages(i,p)\n",
    "        print(imagesVideo.shape)\n",
    "        with open('ImageFrames/Images_Video' + str(i) + '_' + str(p) + '.npy', 'wb') as f:\n",
    "            np.save(f, imagesVideo)\n",
    "        print('Video ' + str(i) + '_' + str(p) + ' saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4db5e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def videoToDilatedThresholdedImages(videoNr, scale_percent):\n",
    "    \"\"\" Gets an array with all image frames of a given video\n",
    "    Args:\n",
    "      videoNr: Number of the video to get the images for\n",
    "      scale_percent: Scale factor to reduce the image for performance reasons\n",
    "      \n",
    "    Returns:\n",
    "      Array with all dilated and thresholded image frames\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calcuate dimensions of scaled images\n",
    "    width = int(1920 * scale_percent / 100)\n",
    "    height = int(1088 * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    \n",
    "    # Load Video\n",
    "    video = cv2.VideoCapture('Data/filmrole' + str(videoNr) + '.avi')\n",
    "    if video.isOpened() == False:\n",
    "        print('Error opening the video file filmrole' + str(videoNr) + '.avi')\n",
    "        \n",
    "    # Iterate through all image frames   \n",
    "    success,image = video.read()\n",
    "    images = np.zeros((3000,height,width),dtype=np.uint8)\n",
    "    count = 0\n",
    "    while success:\n",
    "        \n",
    "        # Resizing image and converting to grayscale\n",
    "        resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "        img_gray = cv2.cvtColor(resized.copy(), cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Dilate image with 3x3 kernel for 3 iterations\n",
    "        kernel = np.ones(shape=(3,3),dtype=np.uint8)\n",
    "        img_dilate = cv2.dilate(src=img_gray,kernel=kernel,iterations=3)\n",
    "        \n",
    "        # Binaray thresholding dilated image with a threshold of 230\n",
    "        ret_img, thresh_img = cv2.threshold(img_dilate, 230, 255, cv2.THRESH_BINARY)\n",
    "        images[count] = thresh_img\n",
    "        success,image = video.read()\n",
    "        count = count+1\n",
    "        \n",
    "    return images[0:count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b116f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2998, 217, 384)\n",
      "Video 1_20 saved\n",
      "(2998, 217, 384)\n",
      "Video 2_20 saved\n",
      "(2999, 217, 384)\n",
      "Video 3_20 saved\n",
      "(2999, 217, 384)\n",
      "Video 4_20 saved\n",
      "(2999, 217, 384)\n",
      "Video 5_20 saved\n",
      "(3000, 217, 384)\n",
      "Video 6_20 saved\n"
     ]
    }
   ],
   "source": [
    "# Save dilated and thresholded images for all six videos in serparate numpy arrays, where all frames are reduced to 20% of their size for performance reasons\n",
    "\n",
    "for i in [1,2,3,4,5,6]:\n",
    "    for p in [20]:\n",
    "        imagesVideo = videoToDilatedThresholdedImages(i,p)\n",
    "        print(imagesVideo.shape)\n",
    "        with open('ImageFrames/Images_DilatedThresholded_Video' + str(i) + '_' + str(p) + '.npy', 'wb') as f:\n",
    "            np.save(f, imagesVideo)\n",
    "        print('Video ' + str(i) + '_' + str(p) + ' saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c8d78f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAnnotationsClassification(videoNr):\n",
    "    \"\"\" Gets an vector with annotations for the classification task ball/no ball for all images of a given vieo\n",
    "    Args:\n",
    "      videoNr: Number of the video to get the annotations for \n",
    "      \n",
    "    Returns:\n",
    "      Vector with annotations for all images of the video with 0 = no ball in image, 1 = ball in image\n",
    "    \"\"\"\n",
    "    \n",
    "    annotations = np.zeros((3001,1),dtype=np.uint8)\n",
    "    with open('Data/cam-' + str(videoNr) + '.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            # No ball in image\n",
    "            if(len(row)==3):\n",
    "                annotations[count] = 0\n",
    "                \n",
    "            # Ball in image\n",
    "            elif(len(row)==1):\n",
    "                annotations[count] = 1\n",
    "            count = count+1\n",
    "    return annotations[1:count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9f78054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations for Video 1 saved\n",
      "Annotations for Video 2 saved\n",
      "Annotations for Video 3 saved\n",
      "Annotations for Video 4 saved\n",
      "Annotations for Video 5 saved\n",
      "Annotations for Video 6 saved\n"
     ]
    }
   ],
   "source": [
    "# Save classification annotations for all six videos\n",
    "\n",
    "for i in [1,2,3,4,5,6]:\n",
    "    annotations = readAnnotationsClassification(i)\n",
    "    with open('AnnotationFiles/Annotations_Classification_Video' + str(i) + '.npy', 'wb') as f:\n",
    "        np.save(f, annotations)\n",
    "    print('Annotations for Video ' + str(i) + ' saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e57ccd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAnnotationsBallPosition(videoNr):\n",
    "    \"\"\" Gets an vector with annotations for the position of the ball (x,y) in all images of a given video\n",
    "    Args:\n",
    "      videoNr: Number of the video to get the annotations for \n",
    "      \n",
    "    Returns:\n",
    "      Vector with annotations for all images of the video with (x,y) position of the ball, or (-1, -1) if no ball\n",
    "    \"\"\"\n",
    "    \n",
    "    annotations = np.zeros((3001,2),dtype=np.int32)\n",
    "    with open('Data/cam-' + str(videoNr) + '.csv', newline='') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            \n",
    "            # No ball in image\n",
    "            if(len(row)==3):\n",
    "                annotations[count,0] = -1\n",
    "                annotations[count,1] = -1\n",
    "                \n",
    "            elif(len(row)==1):\n",
    "                start = row[0].index(',')+1\n",
    "                end = row[0][start:-1].index(',')+start\n",
    "                \n",
    "                # Ball in image\n",
    "                if(row[0][start:end]!='-'):\n",
    "                    annotations[count,0] = int(row[0][start:end])\n",
    "                    annotations[count,1] = int(row[0][end+1:])\n",
    "                    \n",
    "                # No ball in image\n",
    "                else:\n",
    "                    annotations[count,0] = -1\n",
    "                    annotations[count,1] = -1\n",
    "                    \n",
    "            count = count+1\n",
    "            \n",
    "    return annotations[1:count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b17f5e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotations for Video 1 saved\n",
      "Annotations for Video 2 saved\n",
      "Annotations for Video 3 saved\n",
      "Annotations for Video 4 saved\n",
      "Annotations for Video 5 saved\n",
      "Annotations for Video 6 saved\n"
     ]
    }
   ],
   "source": [
    "# Save ball position annotations for all six videos\n",
    "\n",
    "for i in [1,2,3,4,5,6]:\n",
    "    annotations = readAnnotationsBallPosition(i)\n",
    "    with open('AnnotationFiles/Annotations_BallPosition_Video' + str(i) + '.npy', 'wb') as f:\n",
    "        np.save(f, annotations)\n",
    "    print('Annotations for Video ' + str(i) + ' saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee0e7127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAnnotationsClassifyLocation(videoNr, xsplit, ysplit, scale_percent):\n",
    "    \"\"\" Gets an vector with annotations for the discrete grid number in which the ball is for every image frame in the video\n",
    "    Args:\n",
    "      videoNr: Number of the video to get the annotations for \n",
    "      xsplit: Number of discrete grid splits in x direction\n",
    "      ysplit: Number of discrete gird splits in y direction\n",
    "      scale_percent: Scale factor to which the images were reduced\n",
    "      \n",
    "    Returns:\n",
    "      Vector with annotations for the discrete grid number in which the ball is or 0 if no ball\n",
    "    \"\"\"\n",
    "    \n",
    "    # Maximum pixel values for the images that were resized to 10%\n",
    "    xmax = math.floor(1920*(scale_percent/100))\n",
    "    ymax = math.floor(108*(scale_percent/10))\n",
    "    \n",
    "    # Load ball positions\n",
    "    annos = np.load('AnnotationFiles/Annotations_BallPosition_Video' + str(videoNr) + '.npy')\n",
    "                    \n",
    "    classifyLocationAnnotations = np.zeros((annos.shape[0],1),dtype=np.int32)\n",
    "    count=0\n",
    "    for i in annos:\n",
    "        # 0 if no ball in image\n",
    "        if -1 in i:\n",
    "            classifyLocationAnnotations[count]=0\n",
    "                    \n",
    "        # Grid number if ball in imgae\n",
    "        else:\n",
    "            x = np.ceil(i[0]/10/xmax*xsplit)\n",
    "            y = np.ceil(i[1]/10/ymax*ysplit)\n",
    "            classifyLocationAnnotations[count] = (y-1)*xsplit+x\n",
    "        count=count+1\n",
    "    return classifyLocationAnnotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2804cde3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation for Video 1 saved\n",
      "Annotation for Video 2 saved\n",
      "Annotation for Video 3 saved\n",
      "Annotation for Video 4 saved\n",
      "Annotation for Video 5 saved\n",
      "Annotation for Video 6 saved\n"
     ]
    }
   ],
   "source": [
    "# Save annotations for the location classification with 45 columns and 25 rows and a scale factor of 10% for all six videos\n",
    "\n",
    "xsplit = 45\n",
    "ysplit = 25\n",
    "scale_percent = 10\n",
    "import math\n",
    "\n",
    "for i in [1,2,3,4,5,6]:\n",
    "    annotations = readAnnotationsClassifyLocation(i,xsplit,ysplit, scale_percent)\n",
    "    with open('AnnotationFiles/Annotations_ClassifyLocations_Video' + str(i) + '_' + str(xsplit) + '_' + str(ysplit) + '_' + str(scale_percent) +'.npy', 'wb') as f:\n",
    "        np.save(f, annotations)\n",
    "    print('Annotation for Video ' + str(i) + ' saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea835062",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLastNLocactionAnnotations(videoNr, xsplit, ysplit, scale_percent, n):\n",
    "    \"\"\" Gets an vector with annotations for the location grid numbers of the last n images for each image\n",
    "    Args:\n",
    "      videoNr: Number of the video to get the annotations for \n",
    "      n: Number of images to go back from each image\n",
    "      xsplit: Number of discrete grid splits in x direction\n",
    "      ysplit: Number of discrete gird splits in y direction\n",
    "      scale_percent: Scale factor to which the images were reduced\n",
    "      \n",
    "    Returns:\n",
    "      Vector with annotations for the location grid numbers of the last n images for each image\n",
    "    \"\"\"\n",
    "        \n",
    "    # Read annotations for the location classification\n",
    "    annotations = np.load('AnnotationFiles/Annotations_ClassifyLocations_Video' + str(videoNr) + '_' + str(xsplit) + '_' + str(ysplit) + '_' + str(scale_percent) +'.npy')\n",
    "    lastLocationsAnnotations = np.zeros((annotations.shape[0],n),dtype=np.int32)\n",
    "    for i in range(len(annotations)):\n",
    "        lastLocations = np.zeros((n,1))\n",
    "        for j in range(n):\n",
    "            # Set last n locations if n last locations are available\n",
    "            if (i-(j+1)) >= 0:\n",
    "                lastLocations[n-1-j] = annotations[i-(j+1)]\n",
    "                \n",
    "            # For the first n-1 image set the missing locations to the first one\n",
    "            else:\n",
    "                lastLocations[n-1-j] = annotations[0]\n",
    "        lastLocationsAnnotations[i] = lastLocations.ravel()\n",
    "        \n",
    "    return lastLocationsAnnotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8ab1ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation for Video 1 saved\n",
      "Annotation for Video 2 saved\n",
      "Annotation for Video 3 saved\n",
      "Annotation for Video 4 saved\n",
      "Annotation for Video 5 saved\n",
      "Annotation for Video 6 saved\n"
     ]
    }
   ],
   "source": [
    "# Save last 10 annotations for the location classification with 45 columns and 25 rows and a scale factor of 10% for all six videos\n",
    "\n",
    "xsplit = 45\n",
    "ysplit = 25\n",
    "scale_percent = 10\n",
    "n = 10\n",
    "\n",
    "for i in [1,2,3,4,5,6]:\n",
    "    annotations = readLastNLocactionAnnotations(i, xsplit, ysplit, scale_percent, n)\n",
    "    with open('AnnotationFiles/Annotations_LastTenLocations_Video' + str(i) + '_' + str(xsplit) + '_' + str(ysplit) + '_' + str(scale_percent) +'.npy', 'wb') as f:\n",
    "        np.save(f, annotations)\n",
    "    print('Annotation for Video ' + str(i) + ' saved')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
